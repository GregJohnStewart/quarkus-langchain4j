= Ollama

include::./includes/attributes.adoc[]

https://ollama.com/[Ollama] proposes a way to run large language models (LLMs) locally.
You can run many https://ollama.com/library[models] such as LLama2, Mistral, or CodeLlama on your local machine.

[#_prerequisites]
== Prerequisites

To use Ollama, you need to have a running Ollama _server_.
Go to the https://ollama.com/download[Ollama download page] and download the server for your platform.

Once installed, check that Ollama is running using:

[source,shell]
----
> ollama list
----

It may not display any model, which is fine, let's pull the `llama2` model:

[source,shell]
----
> ollama pull llama2
----

WARNING: Models are huge. For example Llama2 is 3.8Gb. Make sure you have enough disk space.

Let's also pull the _default_ embedding model:

[source,shell]
----
> ollama pull nomic-embed-text
----

[NOTE]
.Dev Service
====
If you have Ollama running locally, you do not need a dev service.
However, if you want to use the Ollama dev service, add the following dependency to your project:
[source,xml,subs=attributes+]
----
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-ollama-devservices</artifactId>
    <version>{project-version}</version>
</dependency>
----
Then, in your `application.properties` file add:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.ollama.devservices.model=mistral # It uses orca-mini by default
----
The dev service will start an Ollama server for you, using a docker container. Note that the provisioning can take some time.
====

== Using Ollama

To integrate with models running on Ollama, add the following dependency into your project:

[source,xml,subs=attributes+]
----
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-ollama</artifactId>
    <version>{project-version}</version>
</dependency>
----

If no other LLM extension is installed, link:../ai-services.adoc[AI Services] will automatically utilize the configured Ollama model.

By default, the extension uses `llama2`, the model we pulled in the previous section.
You can change it by setting the `quarkus.langchain4j.ollama.chat-model.model-id` property in the `application.properties` file:

[source,properties,subs=attributes+]
----
# Do not forget to pull the model before using it using `ollama pull <model-id>`
quarkus.langchain4j.ollama.chat-model.model-id=mistral
----

=== Configuration

Several configuration properties are available:

include::includes/quarkus-langchain4j-ollama.adoc[leveloffset=+1,opts=optional]

== Document Retriever and Embedding

Ollama also provides embedding models.
By default, it uses `nomic-embed-text` (make sure you pulled that model as indicated in xref:#_prerequisites[the prerequisites section]).

You can change the default embedding model by setting the `quarkus.langchain4j.ollama.embedding-model.model-id` property in the `application.properties` file:

[source,properties,subs=attributes+]
----
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true

quarkus.langchain4j.ollama.chat-model.model-id=mistral
quarkus.langchain4j.ollama.embedding-model.model-id=mistral
----

If no other LLM extension is installed, retrieve the embedding model as follows:

[source, java]
----
@Inject EmbeddingModel model; // Injects the embedding model
----

However, in general, we recommend using xref:in-process-embedding.adoc[local embedding models], as Ollama embeddings are rather slow.

